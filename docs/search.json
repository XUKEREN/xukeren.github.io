[
  {
    "objectID": "posts/2020-02-02-golf-balls-simulation/index.html#problem",
    "href": "posts/2020-02-02-golf-balls-simulation/index.html#problem",
    "title": "Golf Balls Simulation",
    "section": "Problem",
    "text": "Problem\nAllan Rossman used to live along a golf course and collected the golf balls that landed in his yard. Most of these golf balls had a number on them. Allan tallied the numbers on the first 500 golf balls that landed in his yard one summer.\nSpecifically. he collected the following data: 137 golf balls numbered 1 138 golf balls numbered 2 107 golf balls numbered 3 104 golf balls numbered 4 14 “others” (Either with a different number, or no number at all. We will ignore these other balls for the purposes of this question.)\nQuestion: What is the distribution of these numbers? In particular, are the numbers 1, 2, 3, and 4 equally likely?"
  },
  {
    "objectID": "posts/2020-02-02-golf-balls-simulation/index.html#my-solutions",
    "href": "posts/2020-02-02-golf-balls-simulation/index.html#my-solutions",
    "title": "Golf Balls Simulation",
    "section": "My solutions",
    "text": "My solutions\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n# How many simulations to run?\nNumberOfSims <- 10000\nNumberofBalls <- 500 - 14\n\nset.seed(123) # set the seed for the random number generator - this makes sure the results are reproducible when we are debugging\n\n# create blank vectors to store values\nvec <- vector()\nvec2 <- vector()\nvec3 <- vector()\nvec4 <- vector()\ntheme_set(theme_minimal())\n\n\nSimulation\n\n# compute maximum frequency for each simlation\nfor (j in 1:NumberOfSims) {\n  for (i in 1:NumberofBalls) {\n    vec[i] <- sample(1:4, 1)\n  }\n  vec2[j] <- max(table(vec))\n}\n\n# compute minimum frequency for each simlation\nfor (j in 1:NumberOfSims) {\n  for (i in 1:NumberofBalls) {\n    vec[i] <- sample(1:4, 1)\n  }\n  vec3[j] <- min(table(vec))\n}\n\n# compute range of frequency for each simlation\nrange <- vec2 - vec3\n\n# compute variance of frequency for each simlation\nfor (j in 1:NumberOfSims) {\n  for (i in 1:NumberofBalls) {\n    vec[i] <- sample(1:4, 1)\n  }\n  vec4[j] <- var(table(vec))\n}\n\n\ndf <- cbind(vec2, vec3, vec4, range) %>% as.data.frame()\n\ncolnames(df) <- c(\"max\", \"min\", \"variance\", \"range\")\n\ndim(df)\n\n[1] 10000     4\n\nhead(df)\n\n  max min  variance range\n1 136 112  11.66667    24\n2 136 117 268.33333    19\n3 138 116  43.00000    22\n4 133 108  95.00000    25\n5 129 116  73.66667    13\n6 137 111 107.00000    26\n\n\n\n\nMinimum frequency\n\n# observed vector\nobs <- c(137, 138, 107, 104)\n\n# calculate test statistics\nmin(obs)\n\n[1] 104\n\n# calculate p-value\na <- df %>%\n  filter(min > min(obs)) %>%\n  nrow()\npvalue <- 1 - a / NumberOfSims\npvalue\n\n[1] 0.1481\n\nggplot(aes(x = min), data = df) +\n  geom_histogram(binwidth = 2) +\n  geom_vline(xintercept = min(obs), size = 1.4, color = \"#AFAFFF\") +\n  annotate(\"text\", x = min(obs) - 5, y = 1000, label = \" test statistics = 104 \\n pvalue = 0.1481\", size = 4)\n\n\n\n\n\n\nVariance\n\n# calculate test statistics\nvar(obs)\n\n[1] 343\n\n# calculate p-value\na <- df %>%\n  filter(variance > var(obs)) %>%\n  nrow()\npvalue <- a / NumberOfSims\npvalue\n\n[1] 0.0336\n\n# draw the graph\nggplot(aes(x = variance), data = df) +\n  geom_histogram() +\n  geom_vline(xintercept = var(obs), size = 1.4, color = \"#AFAFFF\") +\n  annotate(\"text\", x = var(obs) + 150, y = 600, label = \" test statistics = 343 \\n pvalue = 0.0336\", size = 4)\n\n\n\n\n\n\nRange\n\n# calculate test statistics\nmax(obs) - min(obs)\n\n[1] 34\n\n# calculate p-value\na <- df %>%\n  filter(range > max(obs) - min(obs)) %>%\n  nrow()\npvalue <- a / NumberOfSims\npvalue\n\n[1] 0.0742\n\n# draw the graph\nggplot(aes(x = range), data = df) +\n  geom_histogram() +\n  geom_vline(xintercept = max(obs) - min(obs), size = 1.4, color = \"#AFAFFF\") +\n  annotate(\"text\", x = max(obs) - min(obs) + 8, y = 600, label = \" test statistics = 34 \\n pvalue = 0.0742\", size = 4)\n\n\n\n\nI tried 3 test statistics using simulation-based hypothesis tests. My null hypothesis here is that the numbers 1, 2, 3, and 4 distribute equally. My alternative hypothesis here is that the numbers 1, 2, 3, and 4 do not distribute equally.\nUsing minimum frequency of ball number among 486 balls as the test statistics, we simulated 10000 times and made a histogram for these 10000 minimum frequency. Our observed test statistics = 104 and our pvalue = 0.1481. Thus, with the significance level of 0.05, we fail to reject the null hypothesis that the numbers 1, 2, 3, and 4 distribute equally likely.\nUsing variance of the frequency of the numbers 1, 2, 3, and 4 as the test statistics, we simulated 10000 times and made a histogram for these 10000 minimum frequency. Our observed test statistics = 343 and our pvalue = 0.0336. Thus, with the significance level of 0.05, we reject the null hypothesis and conclude that the numbers 1, 2, 3, and 4 do not distribute equally.\nUsing range of the frequency of the numbers 1, 2, 3, and 4 as the test statistics, we simulated 10000 times and made a histogram for these 10000 minimum frequency. Our observed test statistics = 34 and our pvalue = 0.0742 Thus, with the significance level of 0.05, we fail to reject the null hypothesis that the numbers 1, 2, 3, and 4 distribute equally."
  },
  {
    "objectID": "posts/2020-05-01-urn-problem/index.html",
    "href": "posts/2020-05-01-urn-problem/index.html",
    "title": "Urn Problem",
    "section": "",
    "text": "Load packages\n\nlibrary(purrr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(knitr)\n\nImaging we want to draw balls from an Urn. Balls are with different colors. Each color has a ‘weight’. When we draw a ball, a given ball is chosen with probability equal to (Weight of that ball)/(total weight of all balls in the Urn).\nNow there are two red balls and one black ball in the urn. If we draw a nonblack ball, we return that ball to urn along with another ball with same color. If we draw a black ball, we return that ball to urn along with another ball that has a color that has not appeared in the urn. Repeat ball drawing process until we have 50 nonblack balls. We repeat this process several times. Assuming all nonblack balls have weight one. Now we want to know the expected number of differnet non black colors in the urn at the end, and the distribution of the numbers of nonblack colors at the end.\nFirst, I wrote an urn function which takes four arguements. First one is the number of colors in the urn at start, second one is the initial number of balls in the urn, the third one represents the number of nonblack balls in the end, and the last one is the weight of the black ball.\nUrn problem wiki page link\n\n# set the random number seed\nset.seed(16)\n\n# Now write a function to simulate the Urn model\n\nUrnSim <- function(NumberOfColorsInUrnAtStart, InitialNBalls, NonBlackBalls, weightofblackball) {\n  # set up the initial state of the urn\n  Urn <- rep(NA, NonBlackBalls + 1)\n  NumberOfColorsUsed <- NumberOfColorsInUrnAtStart\n\n  # we will start with three balls: two \"red\" and one \"black\"\n  # black = 0 and red = 1\n  Urn[1] <- 0\n  Urn[2] <- 1\n  Urn[3] <- 1\n\n  # set up a counter (NumberOfBalls) to keep track of how many balls we have\n  NumberOfBalls <- sum(Urn >= 0, na.rm = TRUE)\n\n  # set-up a loop that pulls a ball from the urn and takes the appropriate action\n  while (NumberOfBalls < (NonBlackBalls + 1)) {\n\n    # set the probability of draw each ball\n    myprob <- c(\n      weightofblackball / (weightofblackball + NumberOfBalls - 1),\n      rep(1 / (weightofblackball + NumberOfBalls - 1), NumberOfBalls - 1)\n    )\n\n    # draw a ball (WhichBall)\n    WhichBall <- Urn[sample(1:NumberOfBalls, size = 1, prob = myprob)]\n\n    # if draw a black ball\n    if (WhichBall == 0) {\n      WhichBall_nonblack <- Urn[sample(2:NumberOfBalls, 1)]\n      # return the ball and change the one's color\n      # the number of color that we have used should be increased\n      # but it does not necessarily mean that the number of colors in our urn has increased\n      NumberOfColorsUsed <- NumberOfColorsUsed + 1\n      # put back that ball with changed color\n      Urn[NumberOfBalls] <- NumberOfColorsUsed\n      # the number of balls did not change\n      NumberOfBalls <- NumberOfBalls\n    } else {\n      # draw a ball which is not black  (whichBall)\n      # return the ball and add another one like it\n      Urn[NumberOfBalls + 1] <- WhichBall\n      # increase the counter of how many balls we have in the urn\n      NumberOfBalls <- NumberOfBalls + 1\n    }\n  }\n  Numberofnonblackcolors <- length(unique(Urn)) - 1\n  Numberofballsofcommonestcolor <- max(table(Urn))\n  distribution <- table(Urn)\n  list(\n    Numberofnonblackcolors = Numberofnonblackcolors,\n    Numberofballsofcommonestcolor = Numberofballsofcommonestcolor,\n    distribution = distribution\n  )\n}\n\n# test the function\nUrnSim(2, 3, 50, 1)\n\n$Numberofnonblackcolors\n[1] 6\n\n$Numberofballsofcommonestcolor\n[1] 20\n\n$distribution\nUrn\n 0  1  3  4  5  6  7 \n 1 13 20  1  7  5  4 \n\n\n\nweight <- c(1:10)\n\nmylist <- weight %>% map(function(x) {\n  NumTrials <- 10000 # how many urns to simulate\n  TrialResults <- rep(0, NumTrials) # somewhere to put the results\n  for (i in 1:length(TrialResults)) {\n    TrialResults[i] <- UrnSim(2, 3, 50, x)$Numberofnonblackcolors\n  }\n  mean(TrialResults)\n})\n\ndf <- data.frame(weight = c(1:10), Numberofnonblackcolors = mylist %>% unlist())\n\ndf %>% kable()\n\n\n\n\nweight\nNumberofnonblackcolors\n\n\n\n\n1\n3.9625\n\n\n2\n6.3618\n\n\n3\n8.4013\n\n\n4\n10.0654\n\n\n5\n11.6262\n\n\n6\n12.9812\n\n\n7\n14.3008\n\n\n8\n15.4330\n\n\n9\n16.4470\n\n\n10\n17.4720\n\n\n\n\nggplot(df, aes(x = weight, y = Numberofnonblackcolors)) +\n  geom_point() +\n  theme_minimal() +\n  geom_smooth(se = FALSE) +\n  scale_x_continuous(breaks = seq(1, 10, 1)) +\n  labs(title = \"the expected number of different (non-black) colors in the Urn \\n  at the end as a function of the weight of the black ball\", x = \"weight of black ball\", y = \"different (non-black) colors  \\n (mean of 10000 simulations)\")\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{xu2020,\n  author = {Keren Xu},\n  editor = {},\n  title = {Urn {Problem}},\n  date = {2020-05-01},\n  url = {https://xukeren.github.io//posts/2020-05-01-urn-problem},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKeren Xu. 2020. “Urn Problem.” May 1, 2020. https://xukeren.github.io//posts/2020-05-01-urn-problem."
  },
  {
    "objectID": "posts/2020-05-01-test-random-pattern/index.html",
    "href": "posts/2020-05-01-test-random-pattern/index.html",
    "title": "Test Random Pattern",
    "section": "",
    "text": "library(purrr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(spatstat)\ntheme_set(theme_minimal())\n\nWe have three grids available showing different patterns. How to test if the pattern is randomly distributed?\nFirst I extracted all points with value of one and coded their coordinates: x-coordinate is the row number and y-coordinate is the column number. Then I got a list of points with their cooordinates. I converted this new data frame to a spatial ppp object. The problem becomes to test for complete spatial randomness (CSR). I chose to use K-function to test CSR. K function is defined as \\(K(h) = \\frac{1}{\\lambda}E\\) \\(\\lambda\\) here represents the intensity of events within distance h. E here is the number of events. In order to estimate the k-function, we can construct a cricle of radius h around each point event(i), count the number of each events(j) that fall inside this circle and repeat these two steps for all points(i) and sum up results. \\(\\hat K(h) = \\frac{R}{n^2}\\sum\\sum_{i\\neq j}\\frac{I_h(d_{ij})}{W_{ij}}\\). Here R is the area, n is the number of points, \\(I_h(d_{ij})\\) equals 1 if \\(d_{ij} \\leq h\\), and 0 otherwise, \\(W_{ij}\\) is the edge correction - the proportion of circumferences of circle = 1 if whole circle in the study area). Under the assumption of CSR, the expected number of events within distance h of an event is \\(K(h) = \\pi h^2\\). \\(K(h) < \\pi h^2\\) if point pattern is regular and \\(K(h) > \\pi h^2\\) if point pattern is clustered. I used envelope function to help calculate the K(h) for observed grid. Also I did 99 simulation - created 99 random patterns, and caculated K(h) for each iteration. Monte Carlo method was used to compute confidence interval. Since there is 99 simiulation, the significance level is 1/(99 + 1) which is 0.01. If the observed K(h) is above the upper bound of the confidence interval, the grid is showing some clustered patterns. If the observed K(h) is within the confidence interval, the grid is showing random pattern. In order to perform 99 simulations, I created a function called random_pattern_generator. First, I randomly sampled 0 or 1 for each random point that I want. Then I converted this vector to a matrix. Points with value of 1 form the random pattern that I desire.\nrandom_pattern_generator <- function(a){\ntemp_v <- sample(c(0, 1), a*a, replace = T)   \ntemp_df <- data.frame(matrix(temp_v , a, a))\nsimdata <- data.frame(x = coordinate(temp_df)$x, y = coordinate(temp_df)$y) %>% filter(!is.na(x)) \nsimdata_rr = ppp(simdata$x, simdata$y, window = owin(c(1,a),c(1,a)))\nsimdata_rr\n}\nHere, grid 1 and grid 2 are showing random patterns and grid 3 is showing clustering.\n\ndf <- read.table(\"./Grid1.txt\", header = FALSE)\n\ndim(df)\n\n[1] 25 25\n\n# create a function to extract coordinates\n\ncoordinate <- function(database) {\n  # create blank vectors to store value\n  x <- rep(0, dim(database)[1] * dim(database)[2])\n  y <- rep(0, dim(database)[1] * dim(database)[2])\n\n  for (i in 1:dim(database)[1]) {\n    for (j in 1:dim(database)[2]) {\n      if (database[i, j] == 0) {\n        x[(i - 1) * dim(database)[2] + j] <- NA\n        y[(i - 1) * dim(database)[2] + j] <- NA\n      }\n      if (database[i, j] == 1) {\n        x[(i - 1) * dim(database)[2] + j] <- i\n        y[(i - 1) * dim(database)[2] + j] <- j\n      }\n    }\n  }\n  list(x = x, y = y)\n}\n\nmydata <- data.frame(x = coordinate(df)$x, y = coordinate(df)$y)\n\nmydata <- mydata %>% filter(!is.na(x))\n\n# visualize the grid\nggplot(mydata, aes(x = x, y = y)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(1, 25, 1)) +\n  scale_y_continuous(breaks = seq(1, 25, 1)) +\n  labs(title = \"Grid 1\")\n\n\n\n# number of points\n\nprint(paste(\"Number of points: \", mydata %>% nrow()))\n\n[1] \"Number of points:  205\"\n\nn <- mydata %>% nrow()\n\n# create a spatial ppp object\nmydata_rr <- ppp(mydata$x, mydata$y, window = owin(c(1, 25), c(1, 25)))\n\n# Creating multiple random point patterns with n points and within the window\n\nset.seed(2020)\n\n# create a function to select n points randomly within the window\n\nrandom_pattern_generator <- function(a) {\n  temp_v <- sample(c(0, 1), a * a, replace = T)\n  temp_df <- data.frame(matrix(temp_v, a, a))\n  simdata <- data.frame(x = coordinate(temp_df)$x, y = coordinate(temp_df)$y) %>% filter(!is.na(x))\n  simdata_rr <- ppp(simdata$x, simdata$y, window = owin(c(1, a), c(1, a)))\n  simdata_rr\n}\n\nex <- expression(random_pattern_generator(25))\n\n# Calculate the upper and lower boundaries\nres <- envelope(mydata_rr, Kest, nsim = 99, simulate = ex, verbose = FALSE, saveall = TRUE, global = TRUE)\n\nres\n\nSimultaneous critical envelopes for K(r)\nand observed value for 'mydata_rr'\nEdge correction: \"iso\"\nObtained from 99 evaluations of user-supplied expression\nEnvelope based on maximum deviation of K(r) from null value (estimated from a \nseparate set of 99 simulations)\nAlternative: two.sided\nSignificance level of simultaneous Monte Carlo test: 1/100 = 0.01\n............................................................\n      Math.label     Description                            \nr     r              distance argument r                    \nobs   hat(K)[obs](r) observed value of K(r) for data pattern\nmmean bar(K)(r)      sample mean of K(r) from simulations   \nlo    hat(K)[lo](r)  lower critical boundary for K(r)       \nhi    hat(K)[hi](r)  upper critical boundary for K(r)       \n............................................................\nDefault plot formula:  .~r\nwhere \".\" stands for 'obs', 'mmean', 'hi', 'lo'\nColumns 'lo' and 'hi' will be plotted as shading (by default)\nRecommended range of argument r: [0, 6]\nAvailable range of argument r: [0, 6]\n\nplot(res)\n\n\n\n\n\ndf <- read.table(\"./Grid2.txt\", header = FALSE)\n\ndim(df)\n\n[1] 25 25\n\ncoordinate <- function(database) {\n  # create blank vectors to store value\n  x <- rep(0, dim(database)[1] * dim(database)[2])\n  y <- rep(0, dim(database)[1] * dim(database)[2])\n\n  for (i in 1:dim(database)[1]) {\n    for (j in 1:dim(database)[2]) {\n      if (database[i, j] == 0) {\n        x[(i - 1) * dim(database)[2] + j] <- NA\n        y[(i - 1) * dim(database)[2] + j] <- NA\n      }\n      if (database[i, j] == 1) {\n        x[(i - 1) * dim(database)[2] + j] <- i\n        y[(i - 1) * dim(database)[2] + j] <- j\n      }\n    }\n  }\n  list(x = x, y = y)\n}\n\nmydata <- data.frame(x = coordinate(df)$x, y = coordinate(df)$y)\n\nmydata <- mydata %>% filter(!is.na(x))\n\n# visualize the grid\nggplot(mydata, aes(x = x, y = y)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(1, 25, 1)) +\n  scale_y_continuous(breaks = seq(1, 25, 1)) +\n  labs(title = \"Grid 2\")\n\n\n\n# number of points\n\nprint(paste(\"Number of points: \", mydata %>% nrow()))\n\n[1] \"Number of points:  221\"\n\nn <- mydata %>% nrow()\n\nlibrary(spatstat)\n\n# create window - a spatial ppp object\nmydata_rr <- ppp(mydata$x, mydata$y, window = owin(c(1, 25), c(1, 25)))\n\n# Creating multiple random point patterns with n points and within the window\n\nset.seed(2020)\n\nrandom_pattern_generator <- function(a) {\n  temp_v <- sample(c(0, 1), a * a, replace = T)\n  temp_df <- data.frame(matrix(temp_v, a, a))\n  simdata <- data.frame(x = coordinate(temp_df)$x, y = coordinate(temp_df)$y) %>% filter(!is.na(x))\n  simdata_rr <- ppp(simdata$x, simdata$y, window = owin(c(1, a), c(1, a)))\n  simdata_rr\n}\n\nex <- expression(random_pattern_generator(25))\n\n# Calculate the upper and lower boundaries\nres <- envelope(mydata_rr, Kest, nsim = 99, simulate = ex, verbose = FALSE, saveall = TRUE, global = TRUE)\n\nres\n\nSimultaneous critical envelopes for K(r)\nand observed value for 'mydata_rr'\nEdge correction: \"iso\"\nObtained from 99 evaluations of user-supplied expression\nEnvelope based on maximum deviation of K(r) from null value (estimated from a \nseparate set of 99 simulations)\nAlternative: two.sided\nSignificance level of simultaneous Monte Carlo test: 1/100 = 0.01\n............................................................\n      Math.label     Description                            \nr     r              distance argument r                    \nobs   hat(K)[obs](r) observed value of K(r) for data pattern\nmmean bar(K)(r)      sample mean of K(r) from simulations   \nlo    hat(K)[lo](r)  lower critical boundary for K(r)       \nhi    hat(K)[hi](r)  upper critical boundary for K(r)       \n............................................................\nDefault plot formula:  .~r\nwhere \".\" stands for 'obs', 'mmean', 'hi', 'lo'\nColumns 'lo' and 'hi' will be plotted as shading (by default)\nRecommended range of argument r: [0, 6]\nAvailable range of argument r: [0, 6]\n\nplot(res)\n\n\n\n\n\ndf <- read.table(\"./Grid3.txt\", header = FALSE)\n\ndim(df)\n\n[1] 25 25\n\ncoordinate <- function(database) {\n  # create blank vectors to store value\n  x <- rep(0, dim(database)[1] * dim(database)[2])\n  y <- rep(0, dim(database)[1] * dim(database)[2])\n\n  for (i in 1:dim(database)[1]) {\n    for (j in 1:dim(database)[2]) {\n      if (database[i, j] == 0) {\n        x[(i - 1) * dim(database)[2] + j] <- NA\n        y[(i - 1) * dim(database)[2] + j] <- NA\n      }\n      if (database[i, j] == 1) {\n        x[(i - 1) * dim(database)[2] + j] <- i\n        y[(i - 1) * dim(database)[2] + j] <- j\n      }\n    }\n  }\n  list(x = x, y = y)\n}\n\nmydata <- data.frame(x = coordinate(df)$x, y = coordinate(df)$y)\n\nmydata <- mydata %>% filter(!is.na(x))\n\n# visualize the grid\nggplot(mydata, aes(x = x, y = y)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(1, 25, 1)) +\n  scale_y_continuous(breaks = seq(1, 25, 1)) +\n  labs(title = \"Grid 3\")\n\n\n\n# number of points\n\nprint(paste(\"Number of points: \", mydata %>% nrow()))\n\n[1] \"Number of points:  213\"\n\nn <- mydata %>% nrow()\n\nlibrary(spatstat)\n\n# create window - a spatial ppp object\nmydata_rr <- ppp(mydata$x, mydata$y, window = owin(c(1, 25), c(1, 25)))\n\n\n# Creating multiple random point patterns with n points and within the window\n\nset.seed(2020)\n\nrandom_pattern_generator <- function(a) {\n  temp_v <- sample(c(0, 1), a * a, replace = T)\n  temp_df <- data.frame(matrix(temp_v, a, a))\n  simdata <- data.frame(x = coordinate(temp_df)$x, y = coordinate(temp_df)$y) %>% filter(!is.na(x))\n  simdata_rr <- ppp(simdata$x, simdata$y, window = owin(c(1, a), c(1, a)))\n  simdata_rr\n}\n\nex <- expression(random_pattern_generator(25))\n\n\n# Calculate the upper and lower boundaries\nres <- envelope(mydata_rr, Kest, nsim = 99, simulate = ex, verbose = FALSE, saveall = TRUE, global = TRUE)\n\nres\n\nSimultaneous critical envelopes for K(r)\nand observed value for 'mydata_rr'\nEdge correction: \"iso\"\nObtained from 99 evaluations of user-supplied expression\nEnvelope based on maximum deviation of K(r) from null value (estimated from a \nseparate set of 99 simulations)\nAlternative: two.sided\nSignificance level of simultaneous Monte Carlo test: 1/100 = 0.01\n............................................................\n      Math.label     Description                            \nr     r              distance argument r                    \nobs   hat(K)[obs](r) observed value of K(r) for data pattern\nmmean bar(K)(r)      sample mean of K(r) from simulations   \nlo    hat(K)[lo](r)  lower critical boundary for K(r)       \nhi    hat(K)[hi](r)  upper critical boundary for K(r)       \n............................................................\nDefault plot formula:  .~r\nwhere \".\" stands for 'obs', 'mmean', 'hi', 'lo'\nColumns 'lo' and 'hi' will be plotted as shading (by default)\nRecommended range of argument r: [0, 6]\nAvailable range of argument r: [0, 6]\n\nplot(res)\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{xu2020,\n  author = {Keren Xu},\n  editor = {},\n  title = {Test {Random} {Pattern}},\n  date = {2020-05-01},\n  url = {https://xukeren.github.io//posts/2020-05-01-test-random-pattern},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKeren Xu. 2020. “Test Random Pattern.” May 1, 2020. https://xukeren.github.io//posts/2020-05-01-test-random-pattern."
  },
  {
    "objectID": "posts/2020-04-23-draw-fractals-from-root-finding-iteration/index.html",
    "href": "posts/2020-04-23-draw-fractals-from-root-finding-iteration/index.html",
    "title": "Zoom Talk Draw Fractals from Root Finding Iterations",
    "section": "",
    "text": "I gave a zoom talk about drawing fractals in R for the LA R Users April Meetup.\nSlides:\nYouTube recording:\nGithub repo:"
  },
  {
    "objectID": "posts/2020-04-23-draw-fractals-from-root-finding-iteration/index.html#samples-codes",
    "href": "posts/2020-04-23-draw-fractals-from-root-finding-iteration/index.html#samples-codes",
    "title": "Zoom Talk Draw Fractals from Root Finding Iterations",
    "section": "Samples codes:",
    "text": "Samples codes:\nUse NewtonRaphson root finding methods to draw fractals for complex functions.\nCreate function newtonraphson\n\n# ftn is the name of a function that has two output including f(x) and f'(x)\n# x0 is the starting point for the algorithm\n# tol is a good stop condition when |f(x)| <= tol for the algorithm, the default here is 1e-9\n# max.iter is a stop condition for the algorithm when n = max.itr\n\nnewtonraphson <- function(ftn, x0, tol = 1e-9, max.iter) {\n  # initialize\n  x <- x0\n  fx <- ftn(x)\n  iter <- 0\n\n  # continue iterating until stopping conditions are met\n  while ((abs(fx[1]) > tol) && (iter < max.iter)) {\n    x <- x - fx[1] / fx[2]\n    fx <- ftn(x)\n    iter <- iter + 1\n    # cat(\"At iteration\", iter, \"value of x is:\", x, \"\\n\")\n  }\n\n  # output depends on the success of the algorithm\n  if (abs(fx[1]) > tol) {\n    # cat(\"Algorithm failed to converge\\n\")\n    return(data.frame(x0, root = NA, iter = NA))\n  } else {\n    # cat(\"Algorithm converged\\n\")\n    return(data.frame(x0, root = x, iter))\n  }\n}\n\nDraw graph for x^3-1\n\nF1 <- function(x) {\n  return(c(x^3 - 1, 3 * (x^2)))\n}\n\n# create complex numbers\nx <- seq(-1, 1, length.out = 500)\ny <- seq(-1, 1, length.out = 500)\nz <- outer(x, 1i * y, \"+\")\n\n# parallel processing using furrr\nplan(multiprocess)\n\ndf <- z %>% future_map_dfr(~ newtonraphson(F1, ., 1e-9, 40), .progress = TRUE)\n\ndf$x <- Re(df$x0)\ndf$y <- Im(df$x0)\n\n# color by iteration\ndf %>% ggplot(aes(x = x, y = y)) +\n  geom_raster(aes(fill = iter), interpolate = TRUE) +\n  scale_fill_gradientn(colors = brewer.pal(12, \"Paired\")) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\ndf %>% ggplot(aes(x = x, y = y)) +\n  geom_raster(aes(fill = iter), interpolate = TRUE) +\n  scale_fill_gradientn(colors = carto.pal(\"multi.pal\")) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\ndf %>% ggplot(aes(x = x, y = y)) +\n  geom_raster(aes(fill = iter), interpolate = TRUE) +\n  scale_fill_gradientn(colors = carto.pal(\"turquoise.pal\")) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\ndf %>% ggplot(aes(x = x, y = y)) +\n  geom_raster(aes(fill = iter), interpolate = TRUE) +\n  scale_fill_gradientn(colors = wes_palette(\"BottleRocket2\")) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\ndf %>% ggplot(aes(x = x, y = y)) +\n  geom_raster(aes(fill = iter), interpolate = TRUE) +\n  scale_fill_gradientn(colors = wes_palette(\"Rushmore1\")) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\nSample codes for Secant method\n\nsecant <- function(ftn, x0, x1, tol = 1e-9, max.iter) {\n  # initialize\n  x_n0 <- x0\n  x_n1 <- x1\n  ftn_n0 <- ftn(x_n0)\n  ftn_n1 <- ftn(x_n1)\n  iter <- 0\n\n  # continue iterating until stopping conditions are met\n  while ((abs(ftn_n1) > tol) && (iter < max.iter)) {\n    x_n2 <- x_n1 - ftn_n1 * (x_n1 - x_n0) / (ftn_n1 - ftn_n0)\n    x_n0 <- x_n1\n    ftn_n0 <- ftn(x_n0)\n    x_n1 <- x_n2\n    ftn_n1 <- ftn(x_n1)\n    iter <- iter + 1\n    # cat(\"At iteration\", iter, \"value of x is:\", x_n1, \"\\n\")\n  }\n\n  return(c(x_n1, iter))\n}"
  },
  {
    "objectID": "posts/2019-11-17-r-talk-at-rladies-pasadena/index.html",
    "href": "posts/2019-11-17-r-talk-at-rladies-pasadena/index.html",
    "title": "R Talk RLadies Pasadena",
    "section": "",
    "text": "I shared my experience in building a personal website by using blogdown at R-Ladies Pasadena November meetup. Thanks Donna and Zhi for organizing this event.\nSlides:\n\n\n\n\nHandout: \nGithub repo: \n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{xu2019,\n  author = {Keren Xu},\n  editor = {},\n  title = {R {Talk} {RLadies} {Pasadena}},\n  date = {2019-11-17},\n  url = {https://xukeren.github.io//posts/2019-11-17-r-talk-at-rladies-pasadena},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKeren Xu. 2019. “R Talk RLadies Pasadena.” November 17,\n2019. https://xukeren.github.io//posts/2019-11-17-r-talk-at-rladies-pasadena."
  },
  {
    "objectID": "posts/2020-08-13-uscbiostats-r-bootcamp-for-scientific-computing-2020-lecture-3/index.html",
    "href": "posts/2020-08-13-uscbiostats-r-bootcamp-for-scientific-computing-2020-lecture-3/index.html",
    "title": "USCbiostats R Bootcamp for Scientific Computing 2020",
    "section": "",
    "text": "I gave a workshop on Automatic Reports with RMarkdown at USCbiostats R Bootcamp for Scientific Computing 2020.\nSlides:\n\n\n\n\nGithub repo: \nYoutube recording: \nRelated tweets:\n\n\n{{% tweet \"1295802310740676608\" %}}\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{xu2020,\n  author = {Keren Xu},\n  editor = {},\n  title = {USCbiostats {R} {Bootcamp} for {Scientific} {Computing} 2020},\n  date = {2020-08-13},\n  url = {https://xukeren.github.io//posts/2020-08-13-uscbiostats-r-bootcamp-for-scientific-computing-2020-lecture-3},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKeren Xu. 2020. “USCbiostats R Bootcamp for Scientific Computing\n2020.” August 13, 2020. https://xukeren.github.io//posts/2020-08-13-uscbiostats-r-bootcamp-for-scientific-computing-2020-lecture-3."
  },
  {
    "objectID": "posts/2020-05-01-a-riddler-problem/index.html",
    "href": "posts/2020-05-01-a-riddler-problem/index.html",
    "title": "A Riddler Problem",
    "section": "",
    "text": "load packages\n\nlibrary(tidyverse)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(knitr)\ntheme_set(theme_minimal())\n\nFirst, I created a function called left_right_center, which takes two arguments Y and X. Y represents the number of 1 dollar bill each player had in the begining of the game. X represents the number of players that participated in our game. The function returns a list with three elements. The first element loop represents how many turns we have to go until only a single person has some money left. The second element center represents the money in the center in the end of our game. The third element is a vector called player_vector, which is showing the situation of each player in the end of the game (the number of bill left for each player).\n\nleft_right_center <- function(Y, X) {\n  player_vector <- rep(Y, X)\n  center <- 0\n  loop <- 0\n\n  while (length(player_vector[player_vector == 0]) < X - 1) {\n    for (i in 1:X) {\n      if (length(player_vector[player_vector == 0]) >= X - 1) {\n        break\n      }\n\n      loop <- loop + 1\n\n      for (j in 1:3) {\n        if (player_vector[i] > 0) {\n          die <- sample(1:6, 1)\n          if (die %in% c(1, 2) & i == 1) {\n            player_vector[length(player_vector)] <- player_vector[length(player_vector)] + 1\n          }\n          if (die %in% c(3, 4) & i == length(player_vector)) {\n            player_vector[1] <- player_vector[1] + 1\n          }\n          if (die %in% c(1, 2) & i != 1) {\n            player_vector[i - 1] <- player_vector[i - 1] + 1\n          }\n          if (die %in% c(3, 4) & i != length(player_vector)) {\n            player_vector[i + 1] <- player_vector[i + 1] + 1\n          }\n          if (die %in% c(5, 6)) {\n            center <- center + 1\n          }\n          player_vector[i] <- player_vector[i] - 1\n        }\n\n        if (player_vector[i] == 0) {\n          break\n        }\n      }\n    }\n  }\n  list(loop = loop, center = center, player_vector = player_vector)\n}\n\nHow long is the game expected to last for six players each starting with three $1 bills? To answer this question, I need to set the function arguement Y = 3 and X = 6. I set the iteration number to 10,000. I created two containers to collect two elements produced by function left_right_center through these 10,000 iterations.\n\nset.seed(2020)\n\nitr <- 10000\n\nloop_holder <- rep(NA, itr)\ncenter_holder <- rep(NA, itr)\n\nfor (i in c(1:itr)) {\n  loop_holder[i] <- left_right_center(3, 6)$loop\n  center_holder[i] <- left_right_center(3, 6)$center\n}\n\nTime to visualize these containers. As we can see from these visualization, the number of turns is quite normally distributed. There are more turns with larger number of bills left on the center.\n\ndf <- data.frame(loop = loop_holder, center = center_holder)\n\ndf %>% ggplot(aes(x = loop)) +\n  geom_histogram(fill = \"#69b3a2\", alpha = 0.8, binwidth = 1)\n\n\n\ndf %>% ggplot(aes(x = center)) +\n  geom_histogram(fill = \"#404080\", alpha = 0.8, binwidth = 1)\n\n\n\n\nThe average number of turns among these 10,000 iterations is 24.63.\n\nmean(df$loop)\n\n[1] 24.6261\n\n# 24.63\n\nHow long is the X players each starting with Y $1 bills?\nI create a simple function which takes two arguments a and b. a represents the number of bills per person and b represents the number of players. I put left_right_center function created previously inside this simple function. Then I did two for loop to check mean loops from each possible combination between an a within 1 and 10, and a b within 2 and 10. I did 1000 iteration for each combination.\n\nset.seed(2020)\n\nsimple_function <- function(a, b) {\n  itr <- 1000\n  loop_holder <- rep(NA, itr)\n  for (i in c(1:itr)) {\n    loop_holder[i] <- left_right_center(a, b)$loop\n  }\n  data.frame(n_bill = a, n_player = b, loop = mean(loop_holder))\n}\n\n\nmylist <- list()\n\nfor (a in c(1:10)) {\n  mylist[[a]] <- c(2:10) %>% map_dfr(~ simple_function(a, .))\n}\n\nmy_df <- mylist %>% bind_rows()\n\nwrite.csv(my_df, \"my_df.csv\")\n\nTime to visualize the results. First, I created a heatmap with number of players on x-axis, and number of bills on y-axis and each block was colored by the number of turns. The higher the number of turn is, the color is more red, whereas the lower the number of turn is, the color is more blue. I also created two line graphs with each showing several panels. The first line graph is showing the the number of turns agains the number of playwers which each panel showing different numbers of bills per person. As we can see, the number of turns is increasing constantly with the increased number of players and the larger the number of bills per person, more straight the line looks like to be. The second line graph is showing the number of turns agins the number of bills person with each panel showing different numbers of players. As we can see, for two players with no more than three bills per person, the number of turns seems to be the same. Other than that, thhe number of turns is increasing constantly with the increased number of bills per person. As we plot these panels togethor on a same line graph, we can see that the magnitude of the correlation between number of bills and turns is getting stronger with the increased number of players. Similarly, the magnitude of the correlation between number of players and turns is getting stronger with the increased number of bills.\n\nmy_df <- read.csv(\"./my_df.csv\") %>% select(-\"X\")\n\nmy_df %>% ggplot(aes(as.factor(n_player), as.factor(n_bill), fill = loop)) +\n  geom_tile() +\n  xlab(\"number of players\") +\n  ylab(\"number of bills\") +\n  scale_fill_gradient(low = \"blue\", high = \"red\")\n\n\n\nmy_df <- my_df %>% mutate(n_player = as.factor(n_player), n_bill = as.factor(n_bill))\n\nmy_df %>% ggplot(aes(x = n_player, y = loop, group = 1)) +\n  geom_line(size = 1) +\n  facet_wrap(~n_bill, scale = \"free\", ncol = 3)\n\n\n\nmy_df %>% ggplot(aes(x = n_bill, y = loop, group = 1)) +\n  geom_line(size = 1) +\n  facet_wrap(~n_player, scale = \"free\", ncol = 3)\n\n\n\nmy_df %>% ggplot(aes(x = n_bill, y = loop, group = n_player)) +\n  geom_line(aes(color = n_player), size = 2)\n\n\n\nmy_df %>% ggplot(aes(x = n_player, y = loop, group = n_bill)) +\n  geom_line(aes(color = n_bill), size = 2)\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{xu2020,\n  author = {Keren Xu},\n  editor = {},\n  title = {A {Riddler} {Problem}},\n  date = {2020-05-01},\n  url = {https://xukeren.github.io//posts/2020-05-01-a-riddler-problem},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKeren Xu. 2020. “A Riddler Problem.” May 1, 2020. https://xukeren.github.io//posts/2020-05-01-a-riddler-problem."
  },
  {
    "objectID": "projects/2021-06-08-Disparities-in-acute-lymphoblastic-leukemia-risk-and-survival-across-the-lifespan-in-the-United-States-of-America/index.html",
    "href": "projects/2021-06-08-Disparities-in-acute-lymphoblastic-leukemia-risk-and-survival-across-the-lifespan-in-the-United-States-of-America/index.html",
    "title": "Disparities in acute lymphoblastic leukemia risk and survival across the lifespan in the United States of America",
    "section": "",
    "text": "author: Keren Xu, Qianxi Feng, Joseph L Wiemels, Adam J de Smith\n\nAbstract:\nAcute lymphoblastic leukemia (ALL) is the most common childhood cancer but is less frequent in adolescents and young adults (AYAs) and is rare among older adults. The 5-year survival of ALL is above 90% in children, but drops significantly in AYAs, and over half of ALL-related deaths occur in older adults. In addition to diagnosis age, the race/ethnicity of patients consistently shows association with ALL incidence and outcomes. Here, we review the racial/ethnic disparities in ALL incidence and outcomes, discuss how these vary across the age spectrum, and examine the potential causes of these disparities. In the United States, the incidence of ALL is highest in Hispanics/Latinos and lowest in Black individuals across all age groups. ALL incidence is rising fastest in Hispanics/Latinos, especially in AYAs. In addition, survival is worse in Hispanic/Latino or Black ALL patients compared to those who are non-Hispanic White. Different molecular subtypes of ALL show heterogeneities in incidence rates and survival outcomes across age groups and race/ethnicity. Several ALL risk variants are associated with genetic ancestry, and demonstrate different risk allele frequencies and/or effect sizes across populations. Moreover, non-genetic factors including socioeconomic status, access to care, and environmental exposures all likely influence the disparities in ALL risk and survival. Further studies are needed to investigate the potential joint effects and interactions of genetic and environmental risk factors. Improving survival in Hispanic/Latino and Black patients with ALL requires advances in precision medicine approaches, improved access to care, and inclusion of more diverse populations in future clinical trials.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/2020-01-30-the-learning-curve-for-shared-decision-making-in-symptomatic-aortic-stenosis/index.html",
    "href": "projects/2020-01-30-the-learning-curve-for-shared-decision-making-in-symptomatic-aortic-stenosis/index.html",
    "title": "The Learning Curve for Shared Decision-making in Symptomatic Aortic Stenosis",
    "section": "",
    "text": "author: Coylewright M, O’Neill E, Sherman A, Gerling M, Adam K, Xu K, Grande S, Dauerman H, Dodge S, Sobti S, Saunders C, Schott S, Elwyn G, Durand M\n\nAbstract:\nImportance Shared decision-making (SDM) is widely advocated for patients with valvular heart disease yet is not integrated into the heart team model for patients with symptomatic aortic stenosis. Decision aids (DAs) have been shown to improve patient-centered outcomes and may facilitate SDM.\nObjective To determine whether the repeated use of a DA by heart teams is associated with greater SDM, along with improved patient-centered outcomes and clinician attitudes about DAs.\nDesign, Setting, and Participants This mixed-methods study included a nonrandomized pre-post intervention and clinician interviews. It was conducted between April 30, 2015, and December 7, 2017, with quantitative analysis performed between January 12, 2017, and May 26, 2017, within 2 academic medical centers in northern New England among 35 patients with symptomatic aortic stenosis who were at high to prohibitive risk for surgery. The qualitative analysis was performed between August 6, 2018, and May 7, 2019. The Severe Aortic Stenosis Decision Aid was delivered by 6 clinicians, with patients choosing between transcatheter aortic valve replacement and medical management.\nMain Outcomes and Measures Clinician SDM performance was measured using the Observer OPTION5 scale with dual-independent coding of audiotaped clinic visits. Previsit and postvisit surveys measured the patient’s knowledge, satisfaction, and decisional conflict. Audiotaped clinician interviews were coded, and qualitative thematic analysis was performed.\nResults Six male clinicians and 35 patients (19 of 34 women [55.9%; 1 survey was missing]; mean [SD] age, 85.8 [7.8] years) participated in the study. Shared decision-making increased stepwise with repeated use of the DA (mean [SD] Observer OPTION5 scores: usual care, 17.9 [7.6]; first use of a DA, 60.5 [30.9]; fifth use of a DA, 79.0 [8.4]; P < .001 for comparison between usual care and fifth use of DA). Multiple uses of the DA were associated with increased patient knowledge (mean difference, 18.0%; 95% CI, 1.2%-34.8%; P = .04) and satisfaction (mean difference, 6.7%; 95% CI, 2.5%-10.8%; P = .01) but not decisional conflict (mean [SD]: usual care, 96.0% [9.4%]; first use of DA, 93.8% [12.5%]; fifth use of DA, 95.0% [11.2%]; P = .60). Qualitative analysis of clinicians’ interviews revealed that clinicians perceived that they used an SDM approach without DAs and that the DA was not well understood by elderly patients. There was infrequent values clarification or discussion of stroke risk.\nConclusion and Relevance In a mixed-methods pilot study, use of a DA for severe aortic stenosis by heart team clinicians was associated with improved SDM and patient-centered outcomes. However, in qualitative interviews, heart team clinicians did not perceive a significant benefit of the DA, and therefore sustained implementation is unlikely. This pilot study of SDM clarifies new research directions for heart teams.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/2022-06-18-Investigating-DNA-methylation-as-a-mediator-of-genetic-risk-in-childhood-acute-lymphoblastic-Leukemia/index.html",
    "href": "projects/2022-06-18-Investigating-DNA-methylation-as-a-mediator-of-genetic-risk-in-childhood-acute-lymphoblastic-Leukemia/index.html",
    "title": "Investigating DNA methylation as a mediator of genetic risk in childhood acute lymphoblastic Leukemia",
    "section": "",
    "text": "author: Keren Xu, Shaobo Li, Priyatama Pandey, Alice Y Kang, Libby M Morimoto, Nicholas Mancuso, Xiaomei Ma, Catherine Metayer, Joseph L Wiemels, Adam J de Smith\n\nAbstract:\nGenome-wide association studies have identified a growing number of single nucleotide polymorphisms (SNPs) associated with childhood acute lymphoblastic leukemia (ALL), yet the functional roles of most SNPs are unclear. Multiple lines of evidence suggest epigenetic mechanisms may mediate the impact of heritable genetic variation on phenotypes. Here, we investigated whether DNA methylation mediates the effect of genetic risk loci for childhood ALL. We performed an epigenome-wide association study (EWAS) including 808 childhood ALL cases and 919 controls from California-based studies using neonatal blood DNA. For differentially methylated CpG positions (DMPs), we next conducted association analysis with 23 known ALL risk SNPs followed by causal mediation analyses addressing the significant SNP-DMP pairs. DNA methylation at CpG cg01139861, in the promoter region of IKZF1, mediated the effects of the intronic IKZF1 risk SNP rs78396808, with the average causal mediation effect (ACME) explaining ~30% of the total effect (ACME P=0.0031). In analyses stratified by self-reported race/ethnicity, the mediation effect was only significant in Latinos, explaining ~41% of the total effect of rs78396808 on ALL risk (ACME P=0.0037). Conditional analyses confirmed the presence of at least three independent genetic risk loci for childhood ALL at IKZF1, with rs78396808 unique to non-European populations. We also demonstrated that the most significant DMP in the EWAS, CpG cg13344587 at gene ARID5B (P=8.61x10−10), was entirely confounded by the ARID5B ALL risk SNP rs7090445. Our findings provide new insights into the functional pathways of ALL risk SNPs and the DNA methylation differences associated with risk of childhood ALL.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/2018-04-24-does-a-telehealth-virtual-consult-including-referring-physicians-specialist-physicians-and-patients-increase-shared-decision-making-for-patients-with-heart-disease/index.html",
    "href": "projects/2018-04-24-does-a-telehealth-virtual-consult-including-referring-physicians-specialist-physicians-and-patients-increase-shared-decision-making-for-patients-with-heart-disease/index.html",
    "title": "Does a Telehealth “Virtual Consult” Including Referring Physicians, Specialist Physicians and Patients Increase Shared Decision Making for Patients With Heart Disease?",
    "section": "",
    "text": "author: Coylewright M, Sherman A, Grande SW, Xu K, Kirk J, Dillon G, O’neill E, Elwyn G\n\nAbstract:\nBackground: Shared decision making (SDM) is highly recommended but difficult to implement for patients with severe heart disease referred for cardiac intervention. This study utilized telehealth (TH) to bring referring physicians and their patients together with a specialist physician to exchange treatment options and patient preferences via a triadic “virtual consult”. This study explores the impact of this innovative approach on SDM and patient decisional conflict.\nMethods: Two cohorts were included: usual care (UC) and TH. UC patients were seen in a clinic with one of 4 participating physicians, and visits were recorded. Telehealth patients met with their local referring physician in the office, and connected remotely with the specialist physician. One of two decision aids (DA) was used: HealthDecision, an electronic health record-integrated DA for atrial fibrillation, or AS Choice, a paper-based DA for severe aortic stenosis. Patient characteristics were collected via surveys. SDM was measured via Observer OPTION-5, a tool used to rate audio or video-taped clinical encounters, with raters’ agreement being assessed by the Bland-Altman analysis (2-rater pairs). Decisional conflict was measured by a 4-item survey, SURE. Data from two cohorts were compared using the Fisher exact test and the Student’s t test.\nResults: Twenty UC visits (5 per physician) were compared with 7 telehealth visits from 4 clinical sites. Patient mean age was 84.3 years and 52% were women. UC patients were older than telehealth patients (87.6 vs. 74.9, p=0.002). Patient decisional conflict was significantly different between the two groups (p=0.02). Telehealth visits had higher OPTION-5 scores than UC visits (99.3 vs. 19.0, p<0.001). (Figure) Rater pairs were used for each observation with evidence of lack of strong agreement in 2 pairs (95% limits of agreement in 3 pairs: [-6.0, 8.8], n=7; [-24.3, 20.7], n=11; [-34.2, 18.7], n=9).\nConclusions: A combined clinical visit with both the referring and specialist physicians, along with their patient in a “virtual consult,” led to decreased patient decisional conflict. Higher OPTION-5 scores were suggested, indicating improvement in the presence of SDM; lack of strong agreement between raters limits this finding and larger studies are needed.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/2019-03-26-timing-of-electronic-health-record-integrated-decision-aid-idea-for-stroke-prevention-in-atrial-fibrillation-matters/index.html",
    "href": "projects/2019-03-26-timing-of-electronic-health-record-integrated-decision-aid-idea-for-stroke-prevention-in-atrial-fibrillation-matters/index.html",
    "title": "Timing of Electronic Health Record Integrated Decision Aid (Idea) for Stroke Prevention in Atrial Fibrillation Matters",
    "section": "",
    "text": "author: Stacey L. Schott, Keren Xu, Julia Berkowitz, Curtis Petersen, Catherine Saunders, Navjot Sobti and Megan Coylewright\n\nAbstract:\nBackground: Although the use of shared decision making (SDM) in cardiology is increasingly recommended, how to best integrate SDM into clinical encounters is unclear. We tested the efficacy of an electronic health record integrated decision aid (IDeA) to determine its impact on knowledge of personalized stroke risk for patients with atrial fibrillation (AF) and its effect on the clinician-patient relationship.\nMethods: A cluster randomized trial of 6 cardiovascular clinicians each paired with 11 new, or previously diagnosed AF patients (N=66) were blindly allocated to use either the IDeA or usual care during an inpatient or outpatient encounter when stroke prevention treatment options were discussed. The primary outcome was knowledge about personalized stroke risk assessed before and after the encounter. Exploratory outcomes included differences in SDM, decisional conflict, confidence, and trust. Time spent on steps in the IDeA was automatically recorded. Semi-structured interviews with clinicians and patients who used the IDeA were conducted.\nResults: There is not enough evidence to suggest this IDeA significantly increased patients’ knowledge of their stroke risk, (OR 3.22 95% CI: 0.69-15.01) and there was no significant difference in SDM, decisional conflict, confidence or trust. Despite training, each clinician used the IDeA differently. Associate providers spent significantly more time on the step displaying % stroke-risk in 1-year (50.22 seconds SD=46.27) than attendings (14.55 seconds SD = 7.87) (p = 0.03), but time spent on any step did not impact patient knowledge. Qualitative analysis revealed the IDeA’s usefulness is dependent upon context, existing patient knowledge and clinician skill. Patients suggested the IDeA may be most helpful at diagnosis.\nConclusion: Previous trials of non-integrated DAs have shown increases in patient knowledge, but in this study, which utilized an integrated DA in real-world clinical encounters, with a majority of previously diagnosed AF patients, these results did not hold true. The qualitative findings help explain this outcome and suggest that timing the delivery of IDeAs matters. Further research should examine IDeA use closer to diagnosis.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/2021-06-10-Personalized-Electronic-Health-Record–Integrated-Decision-Aid-for-Stroke-Prevention-in-Atrial-Fibrillation-A-Small-Cluster-Randomized-Trial-and-Qualitative-Analysis-of-Efficacy-and-Acceptability/index.html",
    "href": "projects/2021-06-10-Personalized-Electronic-Health-Record–Integrated-Decision-Aid-for-Stroke-Prevention-in-Atrial-Fibrillation-A-Small-Cluster-Randomized-Trial-and-Qualitative-Analysis-of-Efficacy-and-Acceptability/index.html",
    "title": "Personalized, Electronic Health Record–Integrated Decision Aid for Stroke Prevention in Atrial Fibrillation, A Small Cluster Randomized Trial and Qualitative Analysis of Efficacy and Acceptability",
    "section": "",
    "text": "author: Stacey L Schott, Julia Berkowitz, Shayne E Dodge, Curtis L Petersen, Catherine H Saunders, Navjot Kaur Sobti, Keren Xu, Megan Coylewright\n\nAbstract:\nBackground: Shared decision-making in cardiology is increasingly recommended to improve patient-centeredness of care. Decision aids can improve patient knowledge and decisional quality but are infrequently used in real-world practice. This mixed-methods study tests the efficacy and acceptability of a decision aid integrated into the electronic health record (Integrated Decision Aid [IDeA]) and delivered by clinicians for patients with atrial fibrillation considering options to reduce stroke risk. We aimed to determine whether the IDeA improves patient knowledge, reduces decisional conflict, and is seen as acceptable by clinicians and patients.\nMethods: A small cluster randomized trial included 6 cardiovascular clinicians and 66 patients randomized either to the IDeA (HealthDecision) or usual care (clinician discretion) during a clinical encounter when stroke prevention treatment options were discussed. The primary outcome was patient knowledge of personalized stroke risk. Exploratory outcomes included decisional conflict, values concordance, trust, the presence of a shared decision-making process, and patient knowledge related to time spent using the IDeA. Additionally, we conducted semistructured interviews with clinicians and patients who used the IDeA were conducted to assess acceptability and predictions of future use.\nResults: The IDeA significantly increased patients’ knowledge of their stroke risk (odds ratio, 3.88 [95% CI, 1.39–10.78]; P<0.01]). Patients had less uncertainty about their final decision (P=0.04). There were no significant differences in values concordance, trust in clinician or shared decision-making. Despite training, each clinician used the IDeA differently. Qualitative analysis revealed patients prefer using the IDeA earlier in their diagnosis. Clinicians were satisfied with the IDeA, yet varied in the contexts in which they planned to use it in the future.\nConclusions: Using an Integrated Decision Aid, or IDeA, increases patient knowledge and lessens uncertainty for decision-making around stroke prevention in atrial fibrillation. Qualitative data provide insight into potential implementation strategies in real-world practice.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/2021-05-21-Epigenetic-Biomarkers-of-Prenatal-Tobacco-Smoke-Exposure-Are-Associated-with-Gene-Deletions-in-Childhood-Acute-Lymphoblastic-Leukemia/index.html",
    "href": "projects/2021-05-21-Epigenetic-Biomarkers-of-Prenatal-Tobacco-Smoke-Exposure-Are-Associated-with-Gene-Deletions-in-Childhood-Acute-Lymphoblastic-Leukemia/index.html",
    "title": "Epigenetic Biomarkers of Prenatal Tobacco Smoke Exposure Are Associated with Gene Deletions in Childhood Acute Lymphoblastic Leukemia",
    "section": "",
    "text": "author: Keren Xu, Shaobo Li, Todd P. Whitehead, Priyatama Pandey, Alice Y. Kang, Libby M. Morimoto, Scott C. Kogan, Catherine Metayer, Joseph L. Wiemels and Adam J. de Smith\n\nAbstract:\nBackground: Parental smoking is implicated in the etiology of acute lymphoblastic leukemia (ALL), the most common childhood cancer. We recently reported an association between an epigenetic biomarker of early-life tobacco smoke exposure at the AHRR gene and increased frequency of somatic gene deletions among ALL cases.\nMethods: Here, we further assess this association using two epigenetic biomarkers for maternal smoking during pregnancy—DNA methylation at AHRR CpG cg05575921 and a recently established polyepigenetic smoking score—in an expanded set of 482 B-cell ALL (B-ALL) cases in the California Childhood Leukemia Study with available Illumina 450K or MethylationEPIC array data. Multivariable Poisson regression models were used to test the associations between the epigenetic biomarkers and gene deletion numbers.\nResults: We found an association between DNA methylation at AHRR CpG cg05575921 and deletion number among 284 childhood B-ALL cases with MethylationEPIC array data, with a ratio of means (RM) of 1.31 [95% confidence interval (CI), 1.02–1.69] for each 0.1 β value reduction in DNA methylation, an effect size similar to our previous report in an independent set of 198 B-ALL cases with 450K array data [meta-analysis summary RM (sRM) = 1.32; 95% CI, 1.10–1.57]. The polyepigenetic smoking score was positively associated with gene deletion frequency among all 482 B-ALL cases (sRM = 1.31 for each 4-unit increase in score; 95% CI, 1.09–1.57).\nConclusions: We provide further evidence that prenatal tobacco-smoke exposure may influence the generation of somatic copy-number deletions in childhood B-ALL.\nImpact: Analyses of deletion breakpoint sequences are required to further understand the mutagenic effects of tobacco smoke in childhood ALL.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/2021-12-11-whole-exome-sequencing-in-multiplex-families-to-identify-novel-aya-classical-hodgkin-lymphoma-predisposition-genes/index.html",
    "href": "projects/2021-12-11-whole-exome-sequencing-in-multiplex-families-to-identify-novel-aya-classical-hodgkin-lymphoma-predisposition-genes/index.html",
    "title": "Whole-Exome Sequencing in Multiplex Families to Identify Novel AYA Classical Hodgkin Lymphoma Predisposition Genes",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/2019-08-05-pragmatic-study-of-clinician-use-of-a-personalized-patient-decision-aid-integrated-into-the-electronic-health-record-an-8-year-experience/index.html",
    "href": "projects/2019-08-05-pragmatic-study-of-clinician-use-of-a-personalized-patient-decision-aid-integrated-into-the-electronic-health-record-an-8-year-experience/index.html",
    "title": "Pragmatic Study of Clinician Use of a Personalized Patient Decision Aid Integrated into the Electronic Health Record: An 8-Year Experience",
    "section": "",
    "text": "author: Megan Coylewright, Jon G. Keevil, Keren Xu, Shayne E. Dodge, Dominick Frosch, and Michael E. Field\n\nAbstract:\nBackground:Patient decision aids (PDAs) facilitate shared decision-making (SDM) and are delivered in a variety of formats, including printed material or instructional videos, and, more recently, web-based tools. Barriers such as time constraints and disruption to clinical workflow are reported to impede usage in routine practice.\nIntroduction:This pragmatic study examines use of PDAs integrated (iPDAs) into the electronic health record (EHR) over an 8-year period.\nMethods:A suite of iPDAs that personalize decision-making was integrated into an academic health system EHR. Clinician use was tracked using patient and clinician encrypted information, enabling identification of clinician types and unique uses for an 8-year period. Clinician feedback was obtained through survey.\nResults:Over 8 years, 1,209 identifiable clinicians used the iPDAs at least once (“aware”). Use increased over time, with 2,415 unique uses in 2010, and 23,456 in 2017. Clinicians who used an iPDA with at least 5 patients (“adopters”), increased by 82 clinicians each year (range 56–108); of clinicians who used the tool once, 54.3% became adopters. Of 261 primary care clinicians, 93.5% were aware, 86.2% were adopters, and 80.5% used the tools in the last 90 days. Clinicians perceived the iPDAs to be convenient, efficient, and encouraging of SDM.\nDiscussion:We demonstrate that use of decision aids integrated into the EHR result in repeated use among clinicians over time and have the potential to overcome barriers to implementation. We noted a high degree of clinician satisfaction, without a sense of increase in visit time.\nConclusion:Integration of PDAs into the EHR results in sustained use. Further research is needed to assess the impact of iPDAs on decisional quality.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/2021-09-24-Accelerated-pigenetic-aging-in-newborns-with-Down-syndrome/index.html",
    "href": "projects/2021-09-24-Accelerated-pigenetic-aging-in-newborns-with-Down-syndrome/index.html",
    "title": "Accelerated Epigenetic Aging in Newborns with Down Syndrome",
    "section": "",
    "text": "author: Keren Xu, Shaobo Li, Ivo S. Muskens, Natalina Elliot, Swe Swe Myint, Priyatama Pandey, Xiaomei Ma, Catherine Metayer, Beth A. Mueller, Kyle M. Walsh, Irene Roberts, Steve Horvath, Joseph L. Wiemels, Adam J. de Smith\nGenerated with posterdown betterland format.\n\nPoster in pdf: link\nCodes: link\n3 minutes narration: link\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/2022-06-06-Accelerated-pigenetic-aging-in-newborns-with-Down-syndrome-paper/index.html",
    "href": "projects/2022-06-06-Accelerated-pigenetic-aging-in-newborns-with-Down-syndrome-paper/index.html",
    "title": "Accelerated Epigenetic Aging in Newborns with Down Syndrome",
    "section": "",
    "text": "author: Keren Xu, Shaobo Li, Ivo S. Muskens, Natalina Elliot, Swe Swe Myint, Priyatama Pandey, Helen M. Hansen, Libby M. Morimoto, Alice Y. Kang, Xiaomei Ma, Catherine Metayer, Beth A. Mueller, Irene Roberts, Kyle M. Walsh, Steve Horvath, Joseph L. Wiemels, Adam J. de Smith\n\nAbstract:\nAccelerated aging is a hallmark of Down syndrome (DS), with adults experiencing early-onset Alzheimer’s disease and premature aging of the skin, hair, and immune and endocrine systems. Accelerated epigenetic aging has been found in the blood and brain tissue of adults with DS but when premature aging in DS begins remains unknown. We investigated whether accelerated aging in DS is already detectable in blood at birth. We assessed the association between age acceleration and DS using five epigenetic clocks in 346 newborns with DS and 567 newborns without DS using Illumina MethylationEPIC DNA methylation array data. We compared two epigenetic aging clocks (DNAmSkinBloodClock and pan-tissue DNAmAge) and three epigenetic gestational age clocks (Haftorn, Knight, and Bohlin) between DS and non-DS newborns using linear regression adjusting for observed age, sex, batch, deconvoluted blood cell proportions, and genetic ancestry. Targeted sequencing of GATA1 was performed in a subset of 184 newborns with DS to identify somatic mutations associated with transient abnormal myelopoiesis. DS was significantly associated with increased DNAmSkinBloodClock (effect estimate = 0.2442, p<0.0001), with an epigenetic age acceleration of 244 days in newborns with DS after adjusting for potential confounding factors (95% confidence interval: 196–292 days). We also found evidence of epigenetic age acceleration associated with somatic GATA1 mutations among newborns with DS (p = 0.015). DS was not associated with epigenetic gestational age acceleration. We demonstrate that accelerated epigenetic aging in the blood of DS patients begins prenatally, with implications for the pathophysiology of immunosenescence and other aging-related traits in DS.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/2017-Perceived-Future-Epilepsy-Risk-Among-Unaffected-Members-of-Multiplex-Epilepsy-Families/index.html",
    "href": "projects/2017-Perceived-Future-Epilepsy-Risk-Among-Unaffected-Members-of-Multiplex-Epilepsy-Families/index.html",
    "title": "Perceived Future Epilepsy Risk Among Unaffected Members of Multiplex Epilepsy Families",
    "section": "",
    "text": "Supervisor: Ruth Ottman\n\n\n\n \n\n\nABSTRACT\nBackground: Genetic influences on the epilepsies are increasingly being emphasized in clinical practice, and genetic testing is becoming a more routine part of clinical care. Hence, understanding of beliefs about epilepsy genetics among unaffected relatives of people with epilepsy is important.\nAims: To investigate, among individuals without epilepsy in multiplex epilepsy families, the impacts on perceived future epilepsy risk of (1) number of relatives with epilepsy and (2) perceived chance of having an epilepsy-related mutation.\nMethods: A self-administered questionnaire was completed by 189 individuals without epilepsy from families containing multiple individuals with epilepsy (average 4 affected per family). Questions asked about the number of people with epilepsy in the family, perceived chance of having an epilepsy-related mutation, and perceived future epilepsy risk “compared with the average person.” Complete data on all three questions were available for 103 participants. Associations among total relatives with epilepsy, perceived chance of having epilepsy-related mutation, and perceived future epilepsy risk were assessed by Poisson regression models using generalized estimating equations to adjust for non-independence among members of each family. Mediation analysis was used to test the degree to which the effect of total relatives with epilepsy on perceived future epilepsy risk was mediated by perceived chance of having a mutation. Stratified analyses and Poisson regression were used to explore interaction between number of relatives with epilepsy and perceived chance of having epilepsy-related mutation.\nResults: Number of affected relatives (≥4 vs. <4) was significantly associated with perceived future epilepsy risk (“more” vs. “the same or less” than the average person) (Prevalence ratio [PR]=1.9, 95% Confidence interval [CI]=1.08-3.22, p=0.02), and with perceived chance of having an epilepsy-related mutation (PR=1.5, 95% CI=1.04-2.04, p=0.03). Perceived chance of having a mutation was also associated with perceived future epilepsy risk (PR=3.4, 95% CI=1.55-7.46, p=0.002). Mediation analysis indicated that number of affected relatives had a significant total effect (PR=1.9, 95% CI=1.06-3.57) on perceived future epilepsy risk and a significant indirect effect, acting through perceived chance of having a mutation (PR=1.3, 95% CI=1.02-1.65). The direct effect of number of affected relatives was not significant (PR=1.6, 95% CI=0.84-2.95). The proportion of the total effect mediated by perceived chance of having a mutation was 41.0% on risk difference scale. Sub-additive interaction between number of relatives with epilepsy and perceived chance of having an epilepsy-related mutation was detected (Relative Excess Risk due to Interaction [RERI] =-1.5, 95% CI=-8.76-5.67).\nConclusions: Our study began with a conceptual mediation model that then brought us to explore interaction between exposure and mediator in their effects on the outcome. The analysis indicated that the relationship between total relatives with epilepsy and perceived future epilepsy risk was mediated by perceived chance of having an epilepsy-related mutation. A nonsignificant sub-additive interaction was detected between total relatives with epilepsy and perceived chance of having an epilepsy-related mutation, suggesting the potential competitive interaction type in our study.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects/2018-03-01-a-learning-curve-for-shared-decision-making-the-impact-of-physician-experience-on-decision-aid-efficacy-in-severe-aortic-stenosis/index.html",
    "href": "projects/2018-03-01-a-learning-curve-for-shared-decision-making-the-impact-of-physician-experience-on-decision-aid-efficacy-in-severe-aortic-stenosis/index.html",
    "title": "a Learning Curve for Shared Decision Making",
    "section": "",
    "text": "author: Coylewright M, O’Neill E, Sherman A, Gerling MJ, Adam K, Xu K, Grande SW, Dauerman HL, Elwyn G\n\nAbstract:\nBackground: The adoption of shared decision making (SDM) in cardiology practice is difficult. This study investigates the impact of physicians’ experience using AS Choice, a decision aid (DA) to help patients with severe aortic stenosis explore options between transcatheter aortic valve replacement and medical therapy, on patient-centered outcomes.\nMethods: Surveys were conducted at 3 time points, with unique patients at each use: T0 (usual care without DA), T1 (physicians used DA for the first time) and T2 (fifth use). Patient knowledge was obtained via pre- and post-visit surveys. Patient satisfaction and SDM were measured by CAHPS and CollaboRATE, respectively. Comparisons were made by paired t-test and Fisher exact test.\nResults: Four physicians and one physician pair (visits held together) participated in the study at two medical centers. Thirty-four patients were included with no patient using DA twice: 25 at T0 (5 per physician), 4 at T1 and 5 at T2. Patients had a mean age of 86.8 years and over half were female. Patients at T2 were significantly older than those at T0 (p=0.03). Patient knowledge increased stepwise from baseline to first use to fifth use, with a significant difference between baseline and multiple uses (p=0.04). Patient satisfaction and SDM also improved with multiple uses (p=0.01 for both). (Figure)\nConclusion: This study suggests a step-wise improvement in patient-centered outcomes with physicians’ greater experience using a DA, and may inform future implementation efforts.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Keren Xu",
    "section": "",
    "text": "1 min\n\n\n\nprojects\n\n\npaper\n\n\n\nepigenome-wide association study, methylation quantitative trait loci, causal mediation\n\n\n\nKeren Xu\n\n\nJun 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nprojects\n\n\npaper\n\n\n\nThe Impact of Physician Experience on Decision Aid Efficacy in Severe Aortic Stenosis\n\n\n\nKeren Xu\n\n\nJun 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nprojects\n\n\nposter\n\n\n\nPoster presentation at the 63rd American Society of Hematology (ASH) annual meeting and exposition, December 11-14, 2021\n\n\n\nKeren Xu\n\n\nNov 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\nprojects\n\n\nposter\n\n\n\nPoster presentation at the (American Society of Human Genetics) ASHG Virtual Meeting, 2021.\n\n\n\nKeren Xu\n\n\nSep 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nprojects\n\n\npaper\n\n\n\nA small cluster randomized trial included 6 cardiovascular clinicians and 66 patients randomized either to the intervention or usual care during a clinical encounter when…\n\n\n\nKeren Xu\n\n\nJun 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nprojects\n\n\npaper\n\n\n\nWe review the racial/ethnic disparities in ALL incidence and outcomes, discuss how these vary across the age spectrum, and examine the potential causes of these disparities.…\n\n\n\nKeren Xu\n\n\nJun 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nprojects\n\n\npaper\n\n\n\nWe assess the association between smoking and gene deletion using two epigenetic biomarkers for maternal smoking during pregnancy—DNA methylation at AHRR CpG cg05575921 and…\n\n\n\nKeren Xu\n\n\nMay 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\nprojects\n\n\npaper\n\n\n\nThis mixed-methods study included a nonrandomized pre-post intervention and clinician interviews.\n\n\n\nKeren Xu\n\n\nJan 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nprojects\n\n\npaper\n\n\n\nThis pragmatic study examines use of PDAs integrated into the electronic health record over an 8-year period.\n\n\n\nKeren Xu\n\n\nAug 5, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nprojects\n\n\nposter\n\n\n\na cluster randomized trial\n\n\n\nKeren Xu\n\n\nMar 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\nprojects\n\n\nposter\n\n\n\nThis study utilized telehealth to bring referring physicians and their patients together with a specialist physician to exchange treatment options and patient preferences…\n\n\n\nKeren Xu\n\n\nApr 24, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\nprojects\n\n\npaper\n\n\n\nThe Impact of Physician Experience on Decision Aid Efficacy in Severe Aortic Stenosis\n\n\n\nKeren Xu\n\n\nMar 11, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\nprojects\n\n\nthesis\n\n\n\nmy master thesis project\n\n\n\nKeren Xu\n\n\nMay 17, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about",
    "section": "",
    "text": "I am a Bioinformatics Scientist at Tempus Labs, with a PhD in Epidemiology from the Keck School of Medicine of University of Southern California, where I studied variant discovery in human DNA sequencing data and (epi)genome-wide association analysis to better understand the genetic and epigenetic risk factors of hematological malignancies, such as child acute lymphoblastic leukemia and Hodgkin’s lymphoma. I completed MPH degree in Epidemiology at the Columbia University Mailman School of Public Health and BS degree in Pharmacy in Shanghai, China. I am interested in genomics data science and data visualization."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Keren Xu",
    "section": "",
    "text": "0 min\n\n\n\npresentations\n\n\nr\n\n\n\na workshop on automatic reports with rmarkdown\n\n\n\nKeren Xu\n\n\nAug 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 min\n\n\n\nnotes\n\n\nr\n\n\n\nan idealized mental exercise\n\n\n\nKeren Xu\n\n\nMay 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7 min\n\n\n\nnotes\n\n\nr\n\n\n\nWe have three grids available showing different patterns. How to test if the pattern is randomly distributed?\n\n\n\nKeren Xu\n\n\nMay 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6 min\n\n\n\nnotes\n\n\nr\n\n\n\nConsider a game of chance called left, right, center. Everyone sits in a circle and begins with some $1 bills …\n\n\n\nKeren Xu\n\n\nMay 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\npresentations\n\n\nr\n\n\n\nuse NewtonRaphson root finding methods to draw fractals for complex functions\n\n\n\nKeren Xu\n\n\nApr 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\nnotes\n\n\nr\n\n\n\nsimulation-based hypothesis tests\n\n\n\nKeren Xu\n\n\nFeb 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 min\n\n\n\npresentations\n\n\nr\n\n\n\nA presentation about building a personal website by using blogdown\n\n\n\nKeren Xu\n\n\nNov 17, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  }
]